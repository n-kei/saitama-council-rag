ä»¥ä¸‹ã®ã‚¢ãƒ—ãƒªç¾¤ã®ä½¿ã„æ–¹ã«ã¤ã„ã¦ã€README.mdã‚’æ›¸ã„ã¦


---


- ãƒ•ã‚©ãƒ«ãƒ€å: .
- ãƒ•ã‚¡ã‚¤ãƒ«å: local_html2text.py
- å†…å®¹:
# local_html2text.py (ã€æœ€çµ‚ä¿®æ­£ç‰ˆã€‘)
import os
import sys
from bs4 import BeautifulSoup
import glob

def convert_html_to_text(html_dir, output_dir):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ã¦ä¿å­˜ã™ã‚‹ã€‚
    ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’åæ˜ ã—ãŸãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆã™ã‚‹ã€‚
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    search_path = os.path.join(html_dir, '**', '*.html')
    html_files = glob.glob(search_path, recursive=True)

    if not html_files:
        print(f"HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{html_dir}' ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    print(f"{len(html_files)} å€‹ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã™...")

    for html_path in html_files:
        try:
            with open(html_path, 'r', encoding='utf-8') as f:
                soup = BeautifulSoup(f.read(), 'lxml')

            content_div = soup.find('div', attrs={'class': 'devsite-article-body'})
            
            if content_div:
                # â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ãŒä¿®æ­£ç®‡æ‰€ã€‘â–¼â–¼â–¼
                
                # 1. å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã‚’å–å¾—
                # ä¾‹: 'reference/spreadsheet/index.html'
                relative_path = os.path.relpath(html_path, html_dir)

                # 2. æ‹¡å¼µå­ã‚’å–ã‚Šé™¤ãã€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåŒºåˆ‡ã‚Šæ–‡å­—ã‚’ãƒã‚¤ãƒ•ãƒ³ã«ç½®æ›
                # ä¾‹: 'reference-spreadsheet-index'
                base_name_without_ext = os.path.splitext(relative_path)[0]
                unique_base_name = base_name_without_ext.replace(os.sep, '-')

                # 3. æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½œæˆ
                # ä¾‹: 'reference-spreadsheet-index.txt'
                output_filename = f"{unique_base_name}.txt"
                output_path = os.path.join(output_dir, output_filename)
                
                # â–²â–²â–²ã€ã“ã“ã¾ã§ãŒä¿®æ­£ç®‡æ‰€ã€‘â–²â–²â–²
                
                with open(output_path, 'w', encoding='utf-8') as out_f:
                    out_f.write(f"Source Path: {html_path}\n\n")
                    out_f.write(content_div.get_text(separator='\n', strip=True))
                
                print(f"å¤‰æ›å®Œäº†: {output_path}")

        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({html_path}): {e}")

# --- GitHub Actions ã¨ ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã®ä¸¡æ–¹ã«å¯¾å¿œ ---
if __name__ == "__main__":
    if len(sys.argv) == 3:
        input_dir = sys.argv[1]
        output_dir = sys.argv[2]
        print(f"--- Processing files from '{input_dir}' to '{output_dir}' (from command line) ---")
        convert_html_to_text(input_dir, output_dir)
    else:
        print("--- Running with default settings for local testing ---")
        
        # --- ãƒ­ãƒ¼ã‚«ãƒ«ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã—ãŸã„å ´åˆã®è¨­å®š ---
        # æ—¢å­˜ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã‚’ä¸€åº¦å‰Šé™¤ã™ã‚‹ã¨ç¢ºå®Ÿã§ã™
        if os.path.exists('gas_docs_txt'):
             import shutil
             shutil.rmtree('gas_docs_txt')
        if os.path.exists('gemini_api_docs_txt'):
             import shutil
             shutil.rmtree('gemini_api_docs_txt')

        # GAS
        convert_html_to_text('gas_docs_html', 'gas_docs_txt')
        # Gemini API
        convert_html_to_text('gemini_api_docs_html', 'gemini_api_docs_txt')


---


- ãƒ•ã‚©ãƒ«ãƒ€å: .
- ãƒ•ã‚¡ã‚¤ãƒ«å: py_wget.py
- å†…å®¹:
# py_wget.py
import os
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

def recursive_download(start_url, output_dir, allowed_domain, wait_time=1):
    """
    æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰å†å¸°çš„ã«HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°ã€‚
    wget --recursive --no-parent --accept=html ã¨ä¼¼ãŸå‹•ä½œã‚’ã—ã¾ã™ã€‚
    """
    urls_to_visit = {start_url}
    visited_urls = set()

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã‘ã‚Œã°ä½œæˆ
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # è¨ªå•ã™ã¹ãURLãŒãªããªã‚‹ã¾ã§ãƒ«ãƒ¼ãƒ—
    while urls_to_visit:
        current_url = urls_to_visit.pop()

        if current_url in visited_urls:
            continue

        print(f"è¨ªå•ä¸­: {current_url}")
        visited_urls.add(current_url)

        try:
            # ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            response = requests.get(current_url, headers={'User-Agent': 'Mozilla/5.0'})
            response.raise_for_status() # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ä¾‹å¤–ã‚’ç™ºç”Ÿ
        except requests.exceptions.RequestException as e:
            print(f"  ã‚¨ãƒ©ãƒ¼: ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ ({e})")
            continue

        # HTMLã‚’è§£æ
        soup = BeautifulSoup(response.content, 'lxml')

        # --- ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãƒ­ã‚¸ãƒƒã‚¯ ---
        # URLã®ãƒ‘ã‚¹éƒ¨åˆ†ã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
        parsed_url = urlparse(current_url)
        # å…ˆé ­ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤
        local_path = parsed_url.path.lstrip('/')
        # ãƒ•ã‚¡ã‚¤ãƒ«åãŒãªã‘ã‚Œã° index.html ã¨ã™ã‚‹
        if local_path.endswith('/'):
            local_path += "index.html"
        elif not os.path.splitext(local_path)[1]: # æ‹¡å¼µå­ãŒãªã„å ´åˆ
             local_path += "/index.html"
        
        file_path = os.path.join(output_dir, local_path)
        
        # ä¿å­˜å…ˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f"  ä¿å­˜å…ˆ: {file_path}")

        # --- æ–°ã—ã„ãƒªãƒ³ã‚¯ã®æ¢ç´¢ãƒ­ã‚¸ãƒƒã‚¯ ---
        # ãƒšãƒ¼ã‚¸å†…ã®ã™ã¹ã¦ã®<a>ã‚¿ã‚°ï¼ˆãƒªãƒ³ã‚¯ï¼‰ã‚’æ¢ã™
        for link in soup.find_all('a', href=True):
            href = link['href']
            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ› (ä¾‹: "../page.html" -> "https://.../page.html")
            new_url = urljoin(current_url, href)
            
            # URLã‹ã‚‰#ä»¥é™ã®ã‚¢ãƒ³ã‚«ãƒ¼ã‚’å‰Šé™¤ (ä¾‹: "...page.html#section1" -> "...page.html")
            new_url = new_url.split('#')[0]

            # --- ãƒ«ãƒ¼ãƒ«ã«åˆè‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ ---
            # 1. è¨±å¯ã•ã‚ŒãŸãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ï¼Ÿ (--domains)
            # 2. ã‚¹ã‚¿ãƒ¼ãƒˆURLã®ãƒ‘ã‚¹é…ä¸‹ã‹ï¼Ÿ (--no-parent)
            # 3. ã¾ã è¨ªå•ã—ã¦ã„ãªã„ã‹ï¼Ÿ
            # 4. HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ï¼Ÿ (æ‹¡å¼µå­ãªã— or .html) (--accept=html)
            if (urlparse(new_url).netloc == allowed_domain and
                new_url.startswith(start_url) and
                new_url not in visited_urls and
                (not os.path.splitext(urlparse(new_url).path)[1] or 
                 os.path.splitext(urlparse(new_url).path)[1] == '.html')):
                
                urls_to_visit.add(new_url)

        # ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†ã«å¾…æ©Ÿ (--wait)
        time.sleep(wait_time)

    print("\nãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    # --- ã“ã“ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã„ã‚µã‚¤ãƒˆã®è¨­å®šã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ ---
    # ã€è¨­å®š1ã€‘Google Apps Scriptã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ

    print("--- Google Apps Script ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ ---")
    recursive_download(
        start_url="https://developers.google.com/apps-script/reference/",
        output_dir="gas_docs_html",
        allowed_domain="developers.google.com"
    )

    print("\n" + "="*50 + "\n")

    # ã€è¨­å®š2ã€‘Gemini APIã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ
    print("--- Gemini API ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ ---")
    recursive_download(
        start_url="https://ai.google.dev/gemini-api/docs/",
        output_dir="gemini_api_docs_html",
        allowed_domain="ai.google.dev"
    )


---


- ãƒ•ã‚©ãƒ«ãƒ€å: .
- ãƒ•ã‚¡ã‚¤ãƒ«å: query_rag.py
- å†…å®¹:
# query_rag.py (äº‹å‰åˆ¤å®šæ©Ÿèƒ½ä»˜ã)
import os
from google import genai
from google.genai import types
from dotenv import load_dotenv

# --- .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€ ---
load_dotenv()
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    raise ValueError("APIã‚­ãƒ¼ãŒ.envãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
client = genai.Client(api_key=api_key) 

# --- ã‚¹ãƒˆã‚¢åã‚’è¨­å®š ---
FILE_SEARCH_STORE_NAME = "fileSearchStores/gas-documentation-rag-store-tkp78e76eio6" # ã‚ãªãŸã®ã‚¹ãƒˆã‚¢åã«è¨­å®šæ¸ˆã¿

# â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ãŒæ–°ã—ã„é–¢æ•°ã€‘â–¼â–¼â–¼
def is_question_about_gas(question: str) -> bool:
    """
    è³ªå•ãŒGoogle Apps Scriptã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹é–¢æ•°
    """
    print("  - è³ªå•å†…å®¹ã‚’åˆ¤å®šä¸­...")
    try:
        # åˆ¤å®šç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = f"""
        ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã¯ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã®ã€ŒGoogle Apps Script (GAS)ã€ã«é–¢é€£ã™ã‚‹å†…å®¹ã§ã™ã‹ï¼Ÿ
        é–¢é€£ã—ã¦ã„ã‚‹å ´åˆã¯ "Yes"ã€é–¢é€£ã—ã¦ã„ãªã„å ´åˆã¯ "No" ã¨ã ã‘ç­”ãˆã¦ãã ã•ã„ã€‚

        è³ªå•: "{question}"
        """
        
        # é«˜é€Ÿãªãƒ¢ãƒ‡ãƒ«ã§åˆ¤å®š
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt,
            config=types.GenerateContentConfig(temperature=0) # å‰µé€ æ€§ã¯ã„ã‚‰ãªã„ã®ã§æ¸©åº¦ã‚’0ã«
        )
        
        # å›ç­”ãŒ"Yes"ã‚’å«ã‚“ã§ã„ã‚‹ã‹ã©ã†ã‹ã§åˆ¤å®š
        print(f"  - åˆ¤å®šçµæœ: {response.text.strip()}")
        return "yes" in response.text.lower()
    except Exception as e:
        print(f"  - åˆ¤å®šä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return False # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨å´ã«å€’ã—ã€å‡¦ç†ã‚’ç¶šè¡Œã—ãªã„
# â–²â–²â–²ã€ã“ã“ã¾ã§ãŒæ–°ã—ã„é–¢æ•°ã€‘â–²â–²â–²

if FILE_SEARCH_STORE_NAME == "ã“ã“ã«ã‚¹ãƒˆã‚¢åã‚’è²¼ã‚Šä»˜ã‘":
    print("ã‚¨ãƒ©ãƒ¼: `FILE_SEARCH_STORE_NAME`ã«å¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
else:
    question = input("GASã«é–¢ã™ã‚‹è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (çµ‚äº†ã™ã‚‹ã«ã¯ Enter ã®ã¿): ")
    
    while question:
        # â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ãŒæ–°ã—ã„ãƒ­ã‚¸ãƒƒã‚¯ã€‘â–¼â–¼â–¼
        if is_question_about_gas(question):
            # è³ªå•ãŒGASã«é–¢é€£ã—ã¦ã„ã‚‹å ´åˆã®ã¿ã€RAGã‚’å®Ÿè¡Œ
            print("\nğŸ¤– AIãŒå›ç­”ã‚’ç”Ÿæˆä¸­...")
            response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents=question,
                config=types.GenerateContentConfig(
                    tools=[
                        types.Tool(
                            file_search=types.FileSearch(
                                file_search_store_names=[FILE_SEARCH_STORE_NAME]
                            )
                        )
                    ]
                )
            )
            print("\n--- å›ç­” ---")
            print(response.text)
            #print("\n--- å¼•ç”¨å…ƒæƒ…å ± ---")
            #print(response.candidates[0].grounding_metadata)
            ###
            # grounding_metadataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
            metadata = response.candidates[0].grounding_metadata
            if metadata:
                print("\n--- å¼•ç”¨å…ƒã®è©³ç´° ---")
                for i, chunk in enumerate(metadata.grounding_chunks):
                    source_file = chunk.retrieved_context.title
                    retrieved_text = chunk.retrieved_context.text
                    print(f"\nã€å¼•ç”¨ {i+1}ã€‘")
                    print(f"  ãƒ•ã‚¡ã‚¤ãƒ«å: {source_file}")
                    print(f"  å†…å®¹ã®å†’é ­: {retrieved_text[:100]}...")
            else:
                # å¼•ç”¨å…ƒãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆï¼ˆRAGãŒæ©Ÿèƒ½ã—ãªã‹ã£ãŸå ´åˆï¼‰
                print("\n--- å¼•ç”¨å…ƒæƒ…å ± ---")
                print("  (ã“ã®å›ç­”ã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã¯å¼•ç”¨ã•ã‚Œã¦ã„ã¾ã›ã‚“)")

            ###
        else:
            # è³ªå•ãŒGASã«é–¢ä¿‚ãªã„å ´åˆã¯ã€å®šå‹æ–‡ã‚’è¿”ã™
            print("\n--- å›ç­” ---")
            print("ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ç§ã¯Google Apps Scriptã«é–¢ã™ã‚‹è³ªå•ã«ã®ã¿ãŠç­”ãˆã§ãã¾ã™ã€‚")
        # â–²â–²â–²ã€ã“ã“ã¾ã§ãŒæ–°ã—ã„ãƒ­ã‚¸ãƒƒã‚¯ã€‘â–²â–²â–²
            
        question = input("\næ¬¡ã®è³ªå•ã‚’ã©ã†ã (çµ‚äº†ã™ã‚‹ã«ã¯ Enter ã®ã¿): ")

print("ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")


---


- ãƒ•ã‚©ãƒ«ãƒ€å: .
- ãƒ•ã‚¡ã‚¤ãƒ«å: requirements.txt
- å†…å®¹:
# requirements.txt

# Gemini APIã‚’æ“ä½œã™ã‚‹ãŸã‚ã®Googleå…¬å¼SDK
google-genai

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
python-dotenv

# Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¨HTMLè§£æã®ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
requests
beautifulsoup4
lxml


---


- ãƒ•ã‚©ãƒ«ãƒ€å: .
- ãƒ•ã‚¡ã‚¤ãƒ«å: setup_rag_store.py
- å†…å®¹:
# setup_rag_store.py (ã€æœ€çµ‚ä¿®æ­£ç‰ˆã€‘)
import os
import time
from google import genai
from dotenv import load_dotenv

# --- .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€ ---
load_dotenv()

# --- ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾— ---
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    raise ValueError("APIã‚­ãƒ¼ãŒ.envãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚'GEMINI_API_KEY=...'ã¨è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚")

# --- Clientã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ ---
client = genai.Client(api_key=api_key) 
doc_directory = "gas_docs_txt"

# --- 1. ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã‚¹ãƒˆã‚¢ã®ä½œæˆ ---
print("ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã‚¹ãƒˆã‚¢ã‚’ä½œæˆã—ã¦ã„ã¾ã™...")
file_search_store = client.file_search_stores.create(
    config={'display_name': 'GAS Documentation RAG Store (new SDK)'}
)

# --- 2. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
print(f"'{doc_directory}' ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™...")
for filename in os.listdir(doc_directory):
    if filename.endswith(".txt"):
        file_path = os.path.join(doc_directory, filename)
        print(f"  - ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­: {filename}")
        
        # æœ€åˆã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ“ä½œã‚’é–‹å§‹
        operation = client.file_search_stores.upload_to_file_search_store(
            file=file_path,
            file_search_store_name=file_search_store.name,
            config={'display_name': filename}
        )
        
        # â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ãŒä¿®æ­£ç®‡æ‰€ã€‘â–¼â–¼â–¼

        # æ“ä½œãŒå®Œäº†ã™ã‚‹ã¾ã§ãƒ«ãƒ¼ãƒ— (å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«æº–æ‹ ã—ãŸã‚·ãƒ³ãƒ—ãƒ«ãªå½¢å¼)
        while not operation.done:
            print("    - å‡¦ç†ä¸­...")
            time.sleep(5)
            
            # operationã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆè‡ªä½“ã‚’æ¸¡ã—ã¦ã€æœ€æ–°ã®çŠ¶æ…‹ã‚’å–å¾—ã™ã‚‹
            operation = client.operations.get(operation)

        # â–²â–²â–²ã€ã“ã“ã¾ã§ãŒä¿®æ­£ç®‡æ‰€ã€‘â–²â–²â–²

print("\nâœ… ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
print("\nğŸ‰ RAGã‚·ã‚¹ãƒ†ãƒ ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
print("ä»¥ä¸‹ã®ã‚¹ãƒˆã‚¢åï¼ˆIDï¼‰ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã€è³ªå•ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚")
print("--------------------------------------------------")
print(file_search_store.name)
print("--------------------------------------------------")


---


- ãƒ•ã‚©ãƒ«ãƒ€å: ./.github/workflows
- ãƒ•ã‚¡ã‚¤ãƒ«å: update-docs.yml
- å†…å®¹:
# .github/workflows/update-docs.yml

name: Update Documentation Files

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å®Ÿè¡Œãƒˆãƒªã‚¬ãƒ¼
on:
  # 1. æ‰‹å‹•å®Ÿè¡Œã‚’å¯èƒ½ã«ã™ã‚‹ (Actionsã‚¿ãƒ–ã® "Run workflow" ãƒœã‚¿ãƒ³)
  workflow_dispatch:

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒå®Ÿè¡Œã™ã‚‹ä¸€é€£ã®ã‚¸ãƒ§ãƒ–
jobs:
  update-docs-job:
    # å®Ÿè¡Œç’°å¢ƒã®æŒ‡å®š (æœ€æ–°ã®Ubuntuã‚’ä½¿ç”¨)
    runs-on: ubuntu-latest

    # ã‚¸ãƒ§ãƒ–ã®å„ã‚¹ãƒ†ãƒƒãƒ—
    steps:
      # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒªãƒã‚¸ãƒˆãƒªã®ã‚³ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯ã‚¢ã‚¦ãƒˆã™ã‚‹
      # ã“ã‚Œã«ã‚ˆã‚Šã€GitHub Actionsã®å®Ÿè¡Œç’°å¢ƒã«ã‚ãªãŸã®ã‚³ãƒ¼ãƒ‰ãŒã‚³ãƒ”ãƒ¼ã•ã‚Œã‚‹
      - name: Checkout repository
        uses: actions/checkout@v4

      # ã‚¹ãƒ†ãƒƒãƒ—2: Pythonç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11' # ä½¿ç”¨ã™ã‚‹Pythonã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³

      # ã‚¹ãƒ†ãƒƒãƒ—3: å¿…è¦ãªPythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹
      # py_wget.py ã‚’å®Ÿè¡Œã—ã¦ã€HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
      - name: Download documentation HTML files
        run: python py_wget.py

      # ã‚¹ãƒ†ãƒƒãƒ—5: HTMLã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹
      # local_html2text.py ã‚’å®Ÿè¡Œã—ã¦ã€TXTãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
      - name: Convert HTML to Text
        run: |
          # GASãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€gemini APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å¤‰æ›
          python local_html2text.py

      # ã‚¹ãƒ†ãƒƒãƒ—6: å¤‰æ›´ãŒã‚ã£ãŸå ´åˆã«ã‚³ãƒŸãƒƒãƒˆï¼†ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹
      - name: Commit and push if there are changes
        run: |
          # Gitã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®š
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          
          # å¤‰æ›´ã‚’ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°
          # ç”Ÿæˆã•ã‚ŒãŸTXTãƒ•ã‚¡ã‚¤ãƒ«ã¨ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸHTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸¡æ–¹ã‚’å¯¾è±¡ã«ã™ã‚‹
          git add gas_docs_txt/ gemini_api_docs_txt/ gas_docs_html/ gemini_api_docs_html/
          
          # å¤‰æ›´ãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèª
          # git diff --staged --quiet ã¯å¤‰æ›´ãŒãªã‘ã‚Œã°çµ‚äº†ã‚³ãƒ¼ãƒ‰0ã€ã‚ã‚Œã°1ã‚’è¿”ã™
          if ! git diff --staged --quiet; then
            # å¤‰æ›´ãŒã‚ã‚‹å ´åˆã®ã¿ã‚³ãƒŸãƒƒãƒˆã¨ãƒ—ãƒƒã‚·ãƒ¥ã‚’è¡Œã†
            git commit -m "docs: Automatically update documentation files"
            git push
          else
            # å¤‰æ›´ãŒãªã„å ´åˆ
            echo "No changes to commit."
          fi